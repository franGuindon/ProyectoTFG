\chapter{Results and Analysis}
\label{chp:results}

The development of Art-FD begins with the setup of the training and testing environment layed out in Figure \ref{fig:trainer}. The first step is to create a dataset. Section \ref{sec:res_dataset} specifies the dataset used. The next step is to test how effective the feature extraction strategy allows the RDF Classifier to perform on the artifact detection task. Section \ref{sec:res_detection} analyzes detection metrics over a series of training parameter experiments. The last step in the solution development is to test and optimize Art-FD's time performance. Section \ref{sec:res_speed} presents the time performance results of the system with respect to the optimizations used.

\section{Expansion and Generation of the Dataset}
\label{sec:res_dataset}

\subsection{Base Videos}
\label{sec:res_base_videos}

The dataset is constructed from three base videos: a walking tour of Valencia in Spain, a webcam capture of the author, and a zoom meeting recording. The walking tour video displays a wide range of scenes and lighting conditions. The webcam capture displays an emulation of several indoor activites. The zoom meeting does not show a great variety of activites, however it provides a realistic scene of a videoconference, where Art-FD might be used. Figure \ref{fig:ds_example} provides an example frame from each base video.

\begin{figure} [!h]
  \centering
  
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{base_21}
    \caption{Frame 21 of Frameset 00 of walking tour. }
    \label{fig:ds_example.a}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{vid2_frame107}
    \caption{Frame 107 of Frameset 13 of webcam capture. }
    \label{fig:ds_example.b}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.32\textwidth}
    \centering
    \includegraphics[width=\textwidth]{vid3_frame18}
    \caption{Frame 18 of Frameset 00 of zoom meeting. }
    \label{fig:ds_example.c}
  \end{subfigure}

  \caption{Example frames from base videos. }
  \label{fig:ds_example}

\end{figure}

The manual inspection of the videos yielded at least 19 locations of interest each. These locations are used for extracting the sets of 200 frames, as described in Section \ref{sec:sol_framesetgen}. Appendix \ref{apx:snip_specs} provides three tables that describe the location and description of each frame set for the walking tour, webcam capture, and zoom call, respectivelly. All the videos have width $1280\,$px and height $720\,$px. All videos use the Quick Time file codec and the targeted H264 video codec.

From \equ{eq:mbw}, the integer number of macroblocks per frame row is
\begin{equation}
  80\text{ macroblocks} = \left\lfloor\frac{1280\text{ pixels}}{16\text{ pixels per macroblock}}\right\rfloor
\end{equation}
and the integer number of macroblocks per frame column is
\begin{equation}
  45\text{ macroblocks} = \left\lfloor\frac{720\text{ pixels}}{16\text{ pixels per macroblock}}\right\rfloor
\end{equation}

The total number of macroblocks in a base video frame is $80\times45=3600\,$macroblocks. However, not all the macroblocks in a frame are used in the dataset. The feature extraction procedure for a macroblock involves pixels that are not within the macroblock borders, so the dataset is constructed from the inner macroblocks within a frame and without considering the bordering macroblocks. Thus, the total number of macroblocks per frame is effectively $(80-2)\times(45-2)=3354\,$macroblocks.

Each macroblock is asociated to an input-output pair in the dataset. The feature extraction procedure generates 132 features for each macroblock. The labeling procedure generates 1 label for each macroblock. For convenience, all labels and features are stored as IEEE 754 single precision values (floats) \cite{Ieee2019}, which have a memory size of 4 bytes or 32 bits. The size of the dataset pair generated by a macroblock is $(132\ \text{features}+1\ \text{label})\times(4\ \text{bytes})=532\ \text{bytes}$ and the memory size of the dataset generated from a base video frame is $(3354\,\text{macroblocks})\times(532\,\text{bytes}) = 1.78\,\text{MB}$, where a megabyte MB represents $10^6$ bytes. The dataset generated from a set of 200 frames has $(3354\,\text{dataset pairs per frame})\times(200\,\text{frames}) = 0.67\,\text{million dataset pairs}$ and has a memory size of $(1.78\,\text{MB per frame})\times(200\,\text{frames}) = 0.357\,\text{GB}$, where a gigabyte GB represents $10^9$ bytes. Table \ref{tab:dataset-info} describes the characteristics of the dataset for each base video, as well as the total dataset. This dataset meets this work's first goal of expanding the dataset to more than 18 frame sets, each with a different video type and 200 frames.

\begin{table}[htbp]
  \caption{General Dataset Information}
  \label{tab:dataset-info}
  \centering
  \begin{tabular}{clccc}
    \hline
    \textbf{ID} & \textbf{Name} & \textbf{\# Sets} & \begin{tabular}{c}\textbf{\# Dataset pairs} \\ \boldsymbol{(\times10^6)}\end{tabular} & \begin{tabular}{c}\textbf{Memory Size} \\ \textbf{(GB)}\end{tabular} \\
    \hline
    1 & Walking Tour & 19 & 12.75 & $6.78$ \\
    2 & Webcam Capture & 20 & 13.42 & $7.14$ \\
    3 & Zoom Call & 20 & 13.42 & $7.14$ \\
    \hline
    & \textbf{TOTAL} & 59 & 39.58 & $21.06$ \\
    \hline
  \end{tabular}
\end{table}

\subsection{Dataset Generation}
\label{sec:res_simulation}

For each base video, the packet loss simulation procedure yields a corresponding video with artifacts. The packet loss probability is set to $0.01$ which, according to \cite{Bienik2023}, affects the user's QoE noticeably.

Each base video and corresponding video with artifacts is synchronized and processed to create frame sets, feature files and label files. During the labeling procedure, the discretization threshold $D_{TH}$ is set to $30$, which corresponds to a confidence threshold of $95\%$ over the accumulated frecuencies of the absolute pixel value differences of all synchronized base videos and video with artifact pairs. Figure \ref{fig:freq} graphs the accumulated frequency over the pixel difference range, with the value at $30$ highlighted. This approach is similar to the one used in \cite{Brenes2022}. 

\begin{figure} [!h]
  \centering
  
  \includegraphics{freq}
  
  \caption{Absolute pixel difference accumulated frequency plot. The point at $(30, 0.95)$ is highlighted.}
  \label{fig:freq}

\end{figure}

Figure \ref{fig:mask_cmp} compares a base video frame, the same frame with artifacts, the corresponding absolute difference frame, and the resulting label mask.

\begin{figure} [!h]
  \centering
  
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{base_21}
    \caption{Base frame.}
    \label{fig:mask_cmp.a}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{artifact_21}
    \caption{Frame with artifacts.}
    \label{fig:mask_cmp.b}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{sub_21}
    \caption{Absolute difference frame.}
    \label{fig:mask_cmp.c}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{mask_21}
    \caption{Label mask with $D_{TH} = 30$.}
    \label{fig:mask_cmp.d}
  \end{subfigure}

  \caption{Comparison between a base frame, a frame with artifacts, an absolute difference frame, and a label mask with $D_{TH}=30$.}
  \label{fig:mask_cmp}

\end{figure}

\section{Detection Performance Results}
\label{sec:res_detection}


\subsection{Training Hardware}
\label{sec:res_hw}

Art-FD system is developed on a 12 core x86 2.6 GHz machine, with 16 GB of memory. However, the dataset creation and RDF training procedures are executed on Amdahl, which has 32 cores and 64 GB of memory. Amdahl allows the procedures to use a larger portion of the dataset at once, which improves the performance of the trained forest models. Amdahl trains 32 trees simultaneously (one of each core), which reduces the training duration.

\subsection{Training Setup}
\label{sec:res_train_setup}

Table \ref{tab:rdf_setup} summarizes the initial RDF training parameter setup. The number of trees in the forest $\NT$ is chosen to match the number of cores on the Amdahl machine. This allows a complete use of the CPU through Ranger's multithreading. The tree depth $\DT$ is kept at $10$ to minimize the forest model memory and time performance cost, while maximizing the number of partitions that each tree tests. The number of split thresholds to try $\STH$ is kept at $50$, which limits each node to $50$ information gain split experiments. The number of features per tree $\FT$ is set to $132$, in order for each tree to test the complete feature vector.

\begin{table}[htbp]
  \caption{RDF Training Parameter Setup}
  \label{tab:rdf_setup}
  \centering
  \begin{tabular}{clc}
  \hline
  \textbf{Parameter} & \textbf{Description} & \textbf{Value} \\
  \hline
  $\NT$ & Trees in forest & 32 \\
  $\DT$ & Maximum tree depth & 10 \\
  $\STH$ & Random Threshold Splits per Tree & 50 \\
  $\FT$ & Random Features per Tree & 132 \\
  \hline
  \end{tabular}
\end{table}

The dataset balance $\DSB$ is set to $0.01$, to match the packet loss probability $P_d$. The first five experiments use a different dataset arrangement each:
 \begin{enumerate}
  \item The 19 walking tour frame sets
  \item The 20 webcam capture frame sets
  \item The 20 zoom call frame sets
  \item 11 frame sets of walking tour and 11 frame sets of webcam capture
  \item 7 frame sets of walking tour, 7 frame sets of webcam capture, and 7 frame sets of zoom call
 \end{enumerate}

These initial experiments use all 132 feature to train each tree. The resulting forest models use specific features at a different frequency than others. An optimization for the feature extraction strategy is to remove the features that are not used. Art-FD does not make use of this optimization. However this work provides a feature analysis for the initial experiments.

The second training setup is designed to run on the Jetson TX2 by reducing the number of trees to $4$. Since the TX2 has effectivelly 4 cores and Ranger runs one thread per tree, then $4$ trees makes complete use of the CPU resources. The number of features per tree is optimized to $11$, according to the recommendation of using the square root of the total number of features ($\lfloor\sqrt{132}\rfloor = 11$) to reduce the correlation between the different tree predictions.

\subsection{Training Results With All 132 Features}
\label{sec:res_train_results_132}

The forest training procedure outputs a forest model for each dataset arrangement. The RDF Classifier generates a prediction vector from the forest model and each corresponding OOB dataset. For each dataset arrangement, the Confusion Analyzer generates the prediction metrics from comparing the prediction vector with the corresponding OOB dataset labels. Table \ref{tab:confusion-matrix} provides training duration, the confusion matrix values, as well as the precision and recall metrics, for forest models trained on each dataset arrangement.

\begin{table}[htbp]
  \caption{Training Results For Five Datasets Using 132 Features}
  \label{tab:confusion-matrix}
  \centering
  \begin{tabular}{rrrrrrrr}
    \hline
    \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{
      \begin{tabular}{c}
        \textbf{Duration} \\ \textbf{(min)}
      \end{tabular}} & \multicolumn{4}{c}{\textbf{Confusion Matrix}} & \multirow{2}{*}{\textbf{Precision}} & \multirow{2}{*}{\textbf{Recall}} \\ 
    & & TP & TN & FP & FN & & \\\hline
    1 & 11.16 & 90829 & 9027884 & 0 & 361 & $1.000$ & $0.996$ \\
    2 & 17.04 & 99857 & 9972661 & 1 & 877 & $1.000$ & $0.991$ \\
    3 & 1.12 & 10908 & 1080090 & 0 & 2 & $1.000$ & $1.000$ \\
    % 05 29 2023 17:17:38
    4 & 16.58 & 144678 & 14366463 & 0 & 437 & $1.000$ & $0.997$ \\
    5 & 21.0 & 119385 & 11852438 & 0 & 336 & $1.000$ & $0.997$ \\ \hline
  \end{tabular}
\end{table}

During each training procedure, Amdahl's CPU profile shows that all 32 CPU cores where running at 100 \% processing power. Additionally, each experiment uses over 40 GB of memory. In particular, experiment 4 is trained on the largest dataset arrangement of 22 frame sets and 7.9 GB. However, Amdahl's CPU profile shows that experiment 4 uses $59.5$ GB of memory. Since the memory consumption during training is over 7 times the size of the dataset, the training procedure cannot use the complete dataset.

The training procedure for each experiment lasts from ten to twenty minutes, with the exception of experiment 3, which lasts just over a minute. This outlier duration suggests that the training algorithm creates shallow trees for experiment 3, which indicates that dataset arrangement 3 is predicted with a small number of subpartitions. Another observation that supports this analysis is that dataset arrangement 3 only considers the zoom call, which does not vary much throughout the video, thus making it more predictable.

Each model achieves precision and recall metrics over $0.99$. These forest models meet this work's second goal of training a classifier that achieves precision and recall metrics over $0.7$.

\subsection{Feature Analysis}
\label{sec:res_feat_analysis}

The forest models from the initial experiment achieve high detection performance with Art-FD's feature selection strategy. However, not all the features are used at the same frequency. Figure \ref{fig:feathist.1} graphs the frequency at which each feature is used in the forest model for experiment 1. Appendix \ref{apx:feat_hist} shows the feature frequency graphs for the remaining 4 experiments. In each graph, the horizontal axis highlights the index ranges for the feature groups specified in Table \ref{tab:feature_vector_spec}.

\begin{figure} [!h]
  \centering
  \includegraphics{fig/feat_hist_1.png}
  \caption{Feature frequency for experiment 1.}
  \label{fig:feathist.1}
\end{figure}

For all experiments, the most frequent features are the two macroblock means for the horizontally and vertically filtered frames (features 2 and 0, respectively). These features are more frequent than any other feature by more than a factor of 2. For each feature group, the border means are the most frequent features, followed by the mean differences. The border variances and variance differences were the least frequent features.

The only feature that is not used in all five experiments is feature 56, which is the right border variance of the vertically filtered frame, for the border of thickness 2. Features 51, 57, 104, and 105 are only used once in all five experiments. These features either correspond to variances or variance differences.

The feature frequency analysis of the five experiments suggests that the variance values and differences do not contribute to the artifact detection task as well as the mean values and differences. A future experiment can test this hypothesis by removing the variance features from the feature extraction strategy. By removing the calculation of variances, Art-FD also removes the calculation of two SSAT for each frame.

\subsection{Training Results for the Jetson TX2 Forest}
\label{sec:res_train_results_132}

The second training setup produces an optimized model for the Jetson TX2. This setup is trained on the same dataset arrangement as experiment 5. This dataset incorporates all three base videos. Table \ref{tab:metrics_small_model} shows the detection performance results for the optimized Art-FD model. Additionally, the table shows the performance results from both the PLDA algorithm (dT2-PLDA) and the RDF classifier (dT2-RDF) used in dispTEC2. The dT2-RDF metrics are obtained by testing on a frame set from the Art-FD dataset. However, the dT2-PLDA metrics are provided by the dispTEC2 project and are obtained from the dispTEC2 dataset, which consists of $1.5\times10^6$ dataset pairs.

\begin{table}[htbp]
  \caption{Performance Comparison Between Art-FD and the dispTEC2 Classifiers}
  \label{tab:metrics_small_model}
  \centering
  \begin{tabular}{rrrrrrr}
    \hline
    \multirow{2}{*}{\textbf{Classifier}} & \multicolumn{4}{c}{\textbf{Confusion Matrix}} & \multirow{2}{*}{\textbf{Precision}} & \multirow{2}{*}{\textbf{Recall}} \\
    & TP & TN & FP & FN & & \\\hline
    Art-FD & 97999 & 9959832 & 53 & 2595 & $0.999$ & $0.974$ \\
    dT2-PLDA & 68 & 120 & 7972 & 0 & $0.008$ & $1.000$ \\
    dT2-RDF & 14202 & 402980 & 6174 & 10417 & $0.697$ & $0.577$ \\
    \hline
  \end{tabular}
\end{table}

The reduction in the number of trees in the forest and the reduction in the number of features per tree lowers the precision and recall with respect to the initial setup. However, the performance metrics remain above $0.97$ and the model meets the Art-FD performance goal. The optimized Art-FD model trains in $2.53$ minutes.

\section{Time Performance Results}
\label{sec:res_speed}

The time performance of the Artifact Detector System at inference time is tested on the TX2 hardware. The compiler is set to maximum optimization, neon intrinsics are activated, and the device runs at maximum power and CPU frequency. Table \ref{tab:times} lists the durations per frame in milliseconds of the main parts of the Artifact Detector System.

\begin{table}[htbp]
  \caption{Artifact Detection System Times per Frame}
  \label{tab:times}
  \centering
  \begin{tabular}{cc}
    \hline
    \textbf{Subsystem} & \textbf{Time ($\mathbf{ms}$)} \\ \hline
    Feature Extractor & 21.867 \\
    RDF Classifier & 1.362 \\
    Mask Generator & 0.940 \\ \hline
    \textbf{Total} & 24.169 \\ \hline
  \end{tabular}
\end{table}

The duration of the classification task per frame is $24.169$ ms, which translates to a framerate of $41.4$ frames per second (fps). This achieves this work's third goal of running under $33$ ms per frame, or over $30$ frames per second. Most software optimizations were implemented for the training procedure of the RDF classifier. Once the system was tested on the TX2 hardware, the time performance goal was met without further optimization.

Table \ref{tab:time_comp} compares the duration and framerate for Art-FD, dT2-PLDA, and dT2-RDF. All classifiers are tested with the Art-FD dataset.

\begin{table}[htbp]
  \caption{Duration and Framerate Comparison Between Art-FD and the dispTEC2 Classifiers}
  \label{tab:time_comp}
  \centering
  \begin{tabular}{ccc}
    \hline
    \textbf{Classifier} & \textbf{Time ($\mathbf{ms}$)} & \textbf{Framerate ($\mathbf{fps}$)} \\ \hline
    Art-FD & 24.169 & 41.375 \\
    dT2-PLDA & 43.408 & 23.037 \\
    dT2-RDF & 37.615 & 26.585 \\ \hline
  \end{tabular}
\end{table}
